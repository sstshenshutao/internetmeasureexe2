{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "a) Read up on the FCC MBA program. In particular, understand how they measure broadband performance,\n",
    "choose samples, validate their collected data, and process/analyze the data for their annual reports. Briefly\n",
    "answer the following questions: Which measurement tests do probes perform? How to they validate their data?\n",
    "Where does the FCC get data on advertised speeds? (2 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "The data was collected from 3,192 volunteer panelists across the United\n",
    "States, the volunteer sample was covering major ISPs in the\n",
    "48 contiguous states across five broadband technologies.\n",
    "\n",
    "The probes did a hardware-based measurement, they did the following\n",
    "measurement tests: \"Download speed\", \"Upload speed\", \"Web browsing\",\n",
    "\"UDP latency\", \"UDP packet loss\", \"Voice over IP\", \"DNS resolution\",\n",
    "\"DNS failures\", \"ICMP latency\", \"ICMP packet loss\", \"UDP Latency under\n",
    "load\", \"Lightweight download speed\", and \"Lightweight upload speed\".\n",
    "\n",
    "The process for the data validation:\n",
    "panelist:\n",
    "• checked the ISP identified by running IP address test in the Whitebox.\n",
    "• detected the throughput speed accurately by flooding each panelist’s connection\n",
    "• confirmed the broadband service tier by ISP\n",
    "• compared the speed tier information to the panelist by SamKnows to ensure accurate tier validation.\n",
    "SamKnows manually:\n",
    "• Verified that the IP address with the ISP ip range.\n",
    "• removed the ISP metrics changed data during the program.\n",
    "• Identified inconsistent throughput with service tier.\n",
    "• Verified the downstream-upstream results with ISP speed tiers\n",
    "\n",
    "Advertised speed data is from ISP."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "b) Download and inspect the validated data from September/October 20182\n",
    ", which were used for the most\n",
    "recent annual MBA report (ninth report). How many measurement probes were part of the program as of\n",
    "Sep/Oct 2018 in total? How many of these probes were excluded from the analysis for the ninth annual report\n",
    "(absolute and relative number)? Visualize the remaining, valid probes on a map of the US based on the\n",
    "information about the unit profile and census blocks data (probes without geocoded data can be left out in the\n",
    "plot). Use different markers for different access technologies. Briefly describe the resulting plot in words, in\n",
    "particular regarding geographical distribution and clusters. Submit your numbers of probes (total, excluded\n",
    "absolute and relative), plotting script, map, and the description of the map. (3 points)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Solution b)\n",
    "\n",
    "The validated data from September/October 2018 is too large, so we didn't integrate the download\n",
    "into the python program.\n",
    "We can download the validated data with \"wget\" in terminal\n",
    "and the extracting of \"tar.gz\" file can be achieved by using \"tar\",\n",
    "we extracted the file into the \"csvdata\" folder.\n",
    "\n",
    "These operations can be achieved by running the bash file:\n",
    "```\n",
    "sh ./download_bash.sh\n",
    "```\n",
    "The folder structure should be like this:\n",
    "```\n",
    ".\n",
    "├── convert_xlsx_csv.py\n",
    "├── csvdata\n",
    "│   ├── curr_dlping.csv\n",
    "│   ├── curr_dns.csv\n",
    "│   ├── curr_httpget.csv\n",
    "│   ├── curr_httpgetmt6.csv\n",
    "│   ├── curr_httpgetmt.csv\n",
    "│   ├── curr_httppost.csv\n",
    "│   ├── curr_httppostmt6.csv\n",
    "│   ├── curr_httppostmt.csv\n",
    "│   ├── curr_ping6.csv\n",
    "│   ├── curr_ping.csv\n",
    "│   ├── curr_udpcloss.csv\n",
    "│   ├── curr_udpjitter.csv\n",
    "│   ├── curr_udplatency6.csv\n",
    "│   ├── curr_udplatency.csv\n",
    "│   ├── curr_ulping.csv\n",
    "│   ├── curr_webget.csv\n",
    "│   └── tmp_ids.csv\n",
    "├── csvexcl\n",
    "│   └── excluded-units-sept2018.xlsx\n",
    "├── csv_id_counter.py\n",
    "├── csvup\n",
    "│   └── Unit-Profile-sept2018.xlsx\n",
    "└── download_bash.sh\n",
    "```\n",
    "Then we can use the 'csv_id_counter.py' to count the \"unit_id\":\n",
    "```\n",
    "python csv_id_counter.py\n",
    "```\n",
    "We got the result: 5589, which means the number of distinct unit-ids in all of the csv-files is 5589.\n",
    "\n",
    "There are 2663 excluded probes in the file [excluded-units-sept2018.xlsx](https://data.fcc.gov/download/measuring-broadband-america/2019/excluded-units-sept2018.xlsx)\n",
    ", we can then run the script 'exclude_probes.py' to generate a dataset which contains the remaining, valid probes.\n",
    "```\n",
    "python exclude_probes.py\n",
    "# result in 'remaining_probes.csv'\n",
    "# number of valid excluded probes: 3128\n",
    "```\n",
    "then we got the number of probes: 3128\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "c) Based on the unit profile data, group the probes by the following columns:\n",
    "\n",
    "• ISP\n",
    "\n",
    "• Technology\n",
    "\n",
    "• State\n",
    "\n",
    "• Census region\n",
    "\n",
    "For each group within the four columns, determine the number of probes, and the average (+ standard deviation)\n",
    "and median advertised download and upload speeds (in Mbps), respectively. Use the upper bound if the stated\n",
    "speeds are listed as an interval. Provide one CSV file for each ISP, technology, state, and census region (i.e., 4\n",
    "in total). Briefly discuss an observation for each CSV file that is interesting or surprising to you. Submit your\n",
    "analysis scripts and created CSV files. (2 points)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}